{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import PIL\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_random_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
    "\n",
    "# Directories\n",
    "localDir = 'images/lego/'\n",
    "\n",
    "# Images\n",
    "imgWidth = 64\n",
    "imgHeight = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make images into multidimensional arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertImageToArray(fileName):\n",
    "    an_image = PIL.Image.open(fileName).convert('L')\n",
    "    image_sequence = an_image.getdata()\n",
    "    return np.array(image_sequence).reshape(imgWidth, imgHeight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IndexToBrickId = {}\n",
    "brickIdImageKeyPairs = []\n",
    "\n",
    "for index, key in enumerate(os.listdir(localDir)):\n",
    "    print(key)\n",
    "    IndexToBrickId[index] = key\n",
    "    \n",
    "    fileList = os.listdir(localDir + key)\n",
    "    listLenStr = str(len(fileList))\n",
    "    for index2, fileName in enumerate(fileList):\n",
    "        filePath = localDir + key + '/' + fileName\n",
    "        print(str(index2+1).zfill(len(listLenStr)) + '/' + listLenStr + ' - ' + filePath + (' ' * 256), end='\\r')\n",
    "        brickIdImageKeyPairs.append((index, convertImageToArray(filePath)))\n",
    "    print(listLenStr + '/' + listLenStr + ' - Finished converting' + (' ' * 256))\n",
    "print('\\nFinished converting all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomize order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brickIdIndexes = []\n",
    "images = []\n",
    "\n",
    "random.shuffle(brickIdImageKeyPairs)\n",
    "for item in brickIdImageKeyPairs:\n",
    "    brickIdIndexes.append(item[0])\n",
    "    images.append(item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(brickIdIndexes[0])\n",
    "print(IndexToBrickId[0])\n",
    "print(images[0] / 255)\n",
    "print(len(images[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "Decision forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentageToTakeTrainFull = int(len(brickIdIndexes) * 0.9)\n",
    "print('Train ' + str(percentageToTakeTrainFull) + '/' + str(len(brickIdIndexes)))\n",
    "X_train_full = np.asarray(images[:percentageToTakeTrainFull])\n",
    "y_train_full = np.asarray(brickIdIndexes[:percentageToTakeTrainFull])\n",
    "X_test = np.asarray(images[percentageToTakeTrainFull:])\n",
    "y_test = np.asarray(brickIdIndexes[percentageToTakeTrainFull:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the size and dimension of the dataset.\n",
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each pixel intensity is represented as a byte (0 to 255).\n",
    "X_train_full.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentageToTakeForValidation = len(X_test)\n",
    "print('Validate ' + str(percentageToTakeForValidation) + '/' + str(len(X_train_full)))\n",
    "\n",
    "# Split the full training set into a validation set and a (smaller) training set,\n",
    "# and scale the pixel intensities down to the 0-1 range and convert them to floats, by dividing by 255.\n",
    "X_valid, X_train = X_train_full[:percentageToTakeForValidation] / 255., X_train_full[percentageToTakeForValidation:] / 255.\n",
    "y_valid, y_train = y_train_full[:percentageToTakeForValidation], y_train_full[percentageToTakeForValidation:]\n",
    "X_test = X_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show random image and class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot an image using Matplotlib's imshow() function, with a gray color map:\n",
    "plt.imshow(X_train[0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the name of the first image in the training set.\n",
    "IndexToBrickId[y_train[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the data\n",
    "Because we want to use the SELU activation function and LeCun weight initializer, we should standardize all the input features to a mean of 0 and a standard deviation of 1. Since each pixel is an input feature, there are 28x28=784 input features, and we need to compute the mean and standard deviation for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean for each pixel.\n",
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_means.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the standard deviation for each pixel.\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "pixel_stds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the inputs to mean 0 and standard deviation 1 to achieve self-normalization with SELU and LeCun.\n",
    "X_train_standardized = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_standardized = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_standardized = (X_test - pixel_means) / pixel_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that the mean is close to 0 for each pixel.\n",
    "X_train_standardized.mean(axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that the standard deviation is close to 1 for each pixel.\n",
    "X_train_standardized.std(axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a model using the Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "# Input layer:\n",
    "# A \"Flatten\" layer converts each input image into a 1-dimensional array.\n",
    "keras.layers.Flatten(input_shape=[imgWidth, imgHeight]),\n",
    "\n",
    "# Hidden layers:\n",
    "# A dense layer is fully connected.\n",
    "keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "\n",
    "# Output layer.\n",
    "# The layer contains one neuron per class (i.e. 10).\n",
    "# Since it is multiclass classification, we should use the softmax activation function.\n",
    "# It will ensure that the estimated probabilities are between 0 and 1, and that the sum\n",
    "# of estimated probabilities for one prediction is 1.\n",
    "# (for binary classification we would have a single output neuron using the logistic activation function).\n",
    "keras.layers.Dense(10, activation=\"softmax\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show information about the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = hidden1.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biases.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model\n",
    "You must at least specify the loss function and the optimizer to use. You can also specify a list of additional metrics to use during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"sparse_categorical_crossentropy\" is the loss function to use for classification when the classes are exclusive.\n",
    "# \"sgd\" means Stochastic Gradient Descent.\n",
    "# \"accuracy\" enables us to measure the accuracy during training and evaluation.\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(momentum=0.9),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EarlyStopping (with rollback to the best model).\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "# Performance scheduling\n",
    "# (multiply the learning rate by a factor when the error stops dropping for a number of epochs, called patience)\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=4)\n",
    "\n",
    "# Train the model with early stopping or performance scheduling or both. Training is much faster when\n",
    "# early stopping is used, but a slightly better accuracy is achieved with performance scheduling alone.\n",
    "history = model.fit(X_train_standardized, y_train, epochs=30,\n",
    "                    validation_data=(X_valid_standardized, y_valid),\n",
    "                    callbacks=[lr_scheduler, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fit() method returns a history object with information about the result of the training.\n",
    "history.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the learning curves.\n",
    "# (The training curves should be shifted half an epoch to the left to be completely comparable with\n",
    "# the validation curves).\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test_standardized, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with probabilities for the first 3 instances in the test set.\n",
    "X_new = X_test_standardized[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions without probabilities.\n",
    "y_pred = model.predict_classes(X_new)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the predictions were coorrect.\n",
    "y_new = y_test[:3]\n",
    "y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare image and classes.\n",
    "newClassList = np.array(list(IndexToBrickId.values()))[y_pred]\n",
    "\n",
    "for i in range(3):\n",
    "    plt.imshow(X_new[i], cmap='gray')\n",
    "    plt.show()\n",
    "    print(newClassList[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
